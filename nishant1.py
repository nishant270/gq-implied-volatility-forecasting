# -*- coding: utf-8 -*-
"""nishant1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KeBEP1EHARM3g5mlaReqb1KSroQkunl6
"""

from google.colab import drive

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Assuming the data is in a folder named 'gq-implied-volatility-forecasting' in your drive
# Please adjust the path if it is different
base_path = '/content/drive/MyDrive/gq-implied-volatility-forecasting'

# Load the main training and test data
try:
    train_eth_df = pd.read_csv(f'{base_path}/train/ETH.csv')
    test_eth_df = pd.read_csv(f'{base_path}/test/ETH.csv')
    submission_df = pd.read_csv(f'{base_path}/submission.csv')

    print("Successfully loaded ETH train, test, and submission data.")

    # Display the first few rows of each dataframe to verify
    print("\nTrain ETH data head:")
    display(train_eth_df.head())

    print("\nTest ETH data head:")
    display(test_eth_df.head())

    print("\nSubmission data head:")
    display(submission_df.head())

except FileNotFoundError:
    print(f"Error: Make sure the path '{base_path}' is correct and the files exist.")
    print("Please provide the correct path to the data files if the default is incorrect.")

# Feature Engineering

# Calculate bid-ask spread
train_eth_df['bid_ask_spread'] = train_eth_df['ask_price1'] - train_eth_df['bid_price1']
test_eth_df['bid_ask_spread'] = test_eth_df['ask_price1'] - test_eth_df['bid_price1']

# Calculate mid-price (already present, but good to keep as a clear feature)
# train_eth_df['mid_price'] = (train_eth_df['bid_price1'] + train_eth_df['ask_price1']) / 2
# test_eth_df['mid_price'] = (test_eth_df['bid_price1'] + test_eth_df['ask_price1']) / 2

# Calculate a simple order book imbalance (OBI)
# Using only the top level for simplicity initially
train_eth_df['obi'] = (train_eth_df['bid_volume1'] - train_eth_df['ask_volume1']) / (train_eth_df['bid_volume1'] + train_eth_df['ask_volume1'])
test_eth_df['obi'] = (test_eth_df['bid_volume1'] - test_eth_df['ask_volume1']) / (test_eth_df['bid_volume1'] + test_eth_df['ask_volume1'])


print("Added bid-ask spread and OBI features to train and test data.")

# Display the first few rows with new features
print("\nTrain ETH data head with new features:")
display(train_eth_df.head())

print("\nTest ETH data head with new features:")
display(test_eth_df.head())

# Data Preprocessing

# Check for missing values
print("Missing values in train_eth_df before handling:")
print(train_eth_df.isnull().sum())

print("\nMissing values in test_eth_df before handling:")
print(test_eth_df.isnull().sum())

# Handle missing values (Example: fill with mean or median, or drop rows)
# For simplicity, let's fill missing numerical values with the mean of the respective column.
# You might want to use a more sophisticated method depending on the data and model.
for col in train_eth_df.select_dtypes(include=['number']).columns:
    if train_eth_df[col].isnull().any():
        mean_val = train_eth_df[col].mean()
        train_eth_df[col].fillna(mean_val, inplace=True)

for col in test_eth_df.select_dtypes(include=['number']).columns:
    if test_eth_df[col].isnull().any():
        mean_val = test_eth_df[col].mean()
        test_eth_df[col].fillna(mean_val, inplace=True)


print("\nMissing values in train_eth_df after handling:")
print(train_eth_df.isnull().sum())

print("\nMissing values in test_eth_df after handling:")
print(test_eth_df.isnull().sum())


# Separate features (X) and target (y) for the training data
# We will use the engineered features and relevant original features.
features = ['mid_price', 'bid_price1', 'bid_volume1', 'ask_price1', 'ask_volume1', 'bid_ask_spread', 'obi'] # Add other relevant features
X_train = train_eth_df[features]
y_train = train_eth_df['label']

# Prepare test data features
X_test = test_eth_df[features]


print("\nShape of X_train:", X_train.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of X_test:", X_test.shape)

import lightgbm as lgb

# Initialize the LightGBM Regressor model
# We use n_estimators for the number of boosting rounds and random_state for reproducibility.
lgbm = lgb.LGBMRegressor(n_estimators=1000, random_state=42)

# Train the model
print("Training the LightGBM model...")
lgbm.fit(X_train, y_train)
print("Model training complete.")

"""### Prediction

Now, we will use the trained `lgbm` model to generate predictions for the implied volatility on the test dataset (`X_test`).
"""

# Make predictions on the test data
predictions = lgbm.predict(X_test)

print("Predictions generated successfully.")

# Display the first few predictions
print("\nFirst 10 predictions:")
print(predictions[:10])

# Create the submission file

# Ensure the number of predictions matches the number of rows in the submission dataframe
if len(predictions) == len(submission_df):
    submission_df['labels'] = predictions
    print("Predictions successfully added to submission dataframe.")

    # Save the submission file
    submission_path = f'{base_path}/submission.csv' # You can specify a different path if needed
    submission_df.to_csv(submission_path, index=False)

    print(f"\nSubmission file created successfully at: {submission_path}")
    print("\nFirst 10 rows of the submission file:")
    display(submission_df.head(10))

else:
    print("Error: The number of predictions does not match the number of rows in the submission dataframe.")
    print("Please check the prediction and submission data shapes.")

import lightgbm as lgb

"""Model Analysis: RMSE"""

from sklearn.metrics import mean_squared_error
import numpy as np

# Calculate RMSE
# Make predictions on the training data to evaluate the model
y_train_pred = lgbm.predict(X_train)

# Calculate the Mean Squared Error
mse = mean_squared_error(y_train, y_train_pred)

# Calculate the Root Mean Squared Error
rmse = np.sqrt(mse)

print(f"Root Mean Squared Error (RMSE) on the training data: {rmse}")